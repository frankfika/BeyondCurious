# Ollama æœ¬åœ°åŒ–éƒ¨ç½²æŒ‡å—

> **ğŸ“ æ¥æº**ï¼šç¬¬3ç«  3.1.2 èŠ‚ã€Œå¦‚ä½•è·å–æœ¬åœ°æ¨¡å‹çš„APIã€
> **ğŸ“„ è¡Œå·**ï¼šç¬¬ 4000-4500 è¡Œ
> **ğŸ“– é¡µç **ï¼šç¬¬ XX é¡µ

---

## ä»€ä¹ˆæ˜¯ Ollamaï¼Ÿ

Ollama æ˜¯ä¸»æµçš„å¤§æ¨¡å‹æœ¬åœ°åŒ–éƒ¨ç½²è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒ macOSã€Linuxã€Windowsï¼Œå¯è¿è¡Œ DeepSeek-R1ã€LLaMAã€Qwen ç­‰å¼€æºæ¨¡å‹ã€‚

## å®‰è£…æ­¥éª¤

### 1. ä¸‹è½½å®‰è£…

è®¿é—® [ollama.com](https://ollama.com) ä¸‹è½½å¯¹åº”ç³»ç»Ÿçš„å®‰è£…åŒ…ã€‚

### 2. éªŒè¯å®‰è£…

```bash
ollama --version
```

### 3. æ‹‰å–æ¨¡å‹

```bash
# æ‹‰å– DeepSeek-R1 7B ç‰ˆæœ¬
ollama pull deepseek-r1:7b

# æ‹‰å– Qwen 2.5 7B ç‰ˆæœ¬
ollama pull qwen2.5:7b
```

### 4. è¿è¡Œæ¨¡å‹

```bash
# å‘½ä»¤è¡Œæé—®æ–¹å¼
ollama run deepseek-r1:7b "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"

# è¿›å…¥å¯¹è¯æ¨¡å¼
ollama run deepseek-r1:7b
```

## æ¨¡å‹å­˜å‚¨è·¯å¾„

- **UNIX-like ç³»ç»Ÿ**ï¼š`~/.ollama/models`
- **Windows ç³»ç»Ÿ**ï¼š`C:\Users\{username}\.ollama\models`

## API ä½¿ç”¨

Ollama é»˜è®¤åœ¨ `http://localhost:11434` æä¾› API æœåŠ¡ã€‚

### Python è°ƒç”¨ç¤ºä¾‹

```python
import requests

response = requests.post('http://localhost:11434/api/generate', json={
    'model': 'deepseek-r1:7b',
    'prompt': 'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ'
})

print(response.json()['response'])
```

## å¸¸ç”¨å‘½ä»¤

```bash
# åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹
ollama list

# æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
ollama show deepseek-r1:7b

# åˆ›å»ºæ¨¡å‹å‰¯æœ¬
ollama copy deepseek-r1:7b my-model

# åˆ é™¤æ¨¡å‹
ollama rm deepseek-r1:7b
```

## ç¡¬ä»¶è¦æ±‚

| ç»„ä»¶ | æœ€ä½è¦æ±‚ | æ¨èé…ç½® |
|------|----------|----------|
| CPU | 4æ ¸å¿ƒ | 8æ ¸å¿ƒ+ |
| å†…å­˜ | 8GB | 16GB+ |
| å­˜å‚¨ | 10GB å¯ç”¨ç©ºé—´ | 50GB+ |
| GPU | æ—  | 4GB+ VRAM |

## æ”¯æŒçš„æ¨¡å‹

- DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B)
- LLaMA 2/3
- Qwen 2.5
- Gemma
- Mistral
- Code Llama

## æ³¨æ„äº‹é¡¹

1. 7B æ¨¡å‹è‡³å°‘éœ€è¦ 8GB å†…å­˜
2. 14B ä»¥ä¸Šæ¨¡å‹å»ºè®®ä½¿ç”¨ GPU
3. é¦–æ¬¡æ‹‰å–æ¨¡å‹éœ€è¦ä¸‹è½½æ—¶é—´
4. API é»˜è®¤åªç›‘å¬æœ¬åœ°ï¼Œéœ€ä¿®æ”¹é…ç½®æ‰èƒ½è¿œç¨‹è®¿é—®
